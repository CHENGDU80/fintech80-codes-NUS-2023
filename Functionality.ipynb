{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2520ea",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "d15f244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#os.environ[\"OPENAI_API_KEY\"]='key_value",
    "import pandas as pd\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import sklearn.cluster\n",
    "from PyPDF2 import PdfReader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import time\n",
    "import os\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "import joblibfrom transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import plotly.express as px\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]='key_value",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_multi = INSTRUCTOR('hkunlp/instructor-base')  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "e399fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"AAPL_news_yahoo_sent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f21ba",
   "metadata": {},
   "source": [
    "#### web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscrapping(company,url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    for i in range(20):\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, 99999);\")\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html\")   \n",
    "    df = pd.DataFrame(columns=['title','desc','url'])\n",
    "    title = []\n",
    "    url = []\n",
    "    for i in soup.findAll(\"h3\", class_=\"Mb(5px)\"):\n",
    "        title.append(i.text)\n",
    "        url.append(\"https://finance.yahoo.com\"+i.findChild('a').get('href'))\n",
    "    desc = []\n",
    "    for j in soup.findAll(\"p\", {\"class\":[\"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(3,57px) LineClamp(3,51px)--sm1024 M(0)\", \"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0)\", \"Fz(14px) Lh(19px) Fz(13px)--sm1024 Lh(17px)--sm1024 LineClamp(2,38px) LineClamp(2,34px)--sm1024 M(0) D(n)--sm1024 Bxz(bb) Pb(2px)\"]}):\n",
    "        desc.append(j.text)\n",
    "    date = []\n",
    "    for k in soup.findAll('div', class_='C(#959595) Fz(11px) D(ib) Mb(6px)'):\n",
    "        date.append(k.text)\n",
    "\n",
    "    df['title'] = title\n",
    "    df['desc'] = desc\n",
    "    df['url'] = url\n",
    "    df['source'] = date\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime(datetime.date.today())\n",
    "def create_date(row):\n",
    "    temp = row['source']\n",
    "    if 'minute' in temp or 'hour' in temp:\n",
    "        return today\n",
    "    elif 'yesterday' in temp:\n",
    "        return today - pd.to_timedelta(1,'days')\n",
    "    elif '2' in temp:\n",
    "        return today - pd.to_timedelta(2,'days')\n",
    "    elif '3' in temp:\n",
    "        return today - pd.to_timedelta(3,'days')    \n",
    "    elif '4' in temp:\n",
    "        return today - pd.to_timedelta(4,'days')    \n",
    "    elif '5' in temp:\n",
    "        return today - pd.to_timedelta(5,'days')\n",
    "    elif '6' in temp:\n",
    "        return today - pd.to_timedelta(6,'days')\n",
    "    elif '7' in temp:\n",
    "        return today - pd.to_timedelta(7,'days')    \n",
    "    elif '8' in temp:\n",
    "        return today - pd.to_timedelta(8,'days')    \n",
    "    elif '9' in temp:\n",
    "        return today - pd.to_timedelta(9,'days')\n",
    "    elif '10' in temp:\n",
    "        return today - pd.to_timedelta(10,'days')\n",
    "    elif '11' in temp:\n",
    "        return today - pd.to_timedelta(11,'days')    \n",
    "    elif '12' in temp:\n",
    "        return today - pd.to_timedelta(12,'days')    \n",
    "    elif '13' in temp:\n",
    "        return today - pd.to_timedelta(13,'days')\n",
    "df['date'] = df.apply(create_date,axis=1)\n",
    "df['source'] = df['source'].str.split('â€¢').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40ff49",
   "metadata": {},
   "source": [
    "### Finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert(df):\n",
    "    df['feed'] = df['title'] + \". \" + df['desc']\n",
    "    sent_val = list()\n",
    "    conf_score = list()\n",
    "    pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "    for x in df['feed']:\n",
    "        p = pipe(x)[0]\n",
    "        val = p['label']\n",
    "        conf = p['score']\n",
    "        print(x, '----', val)\n",
    "        print('#######################################################')\n",
    "        sent_val.append(val)\n",
    "        conf_score.append(conf)\n",
    "    df['sentiment'] = sent_val\n",
    "    df['confidence'] = conf_score\n",
    "    df = df.drop(columns=['feed'])   \n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be32bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "news_text = \"Markets will gyrate right up until the Fed announces its interest-rate decision. But don't forget: Apple reports quarterly results on Thursday.\"\n",
    "sentiment_result = sentiment_analysis(news_text)\n",
    "\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77a3fb",
   "metadata": {},
   "source": [
    "## impact analysis for negative and positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "28bc01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neagtive_impact_analysis(df):\n",
    "    df[df[\"date\"]=='2023-10-29']\n",
    "    negative_news=df[df['sentiment']=='negative']\n",
    "    negative_news = negative_news.sort_values(by='confidence', ascending=False)\n",
    "    negative_news=negative_news.head(1)\n",
    "    negative_news_list=negative_news['desc'].tolist()\n",
    "    negative_impact_list=[]\n",
    "    negative_impact_dict={}                 \n",
    "    for i in negative_news_list:\n",
    "        \n",
    "        #print(str(i))\n",
    "        #demo_template='''Please clasify this content {docs} into three categories: financial-related contents, operational-related contents, brand reputational-related contents, and others.'''\n",
    "        #demo_template='''Given that {impact}, what impact can this lead on NVIDIA Corporation (NVDA)? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        #demo_template='''Given that {impact}, what impact can this lead on Apple Inc(AAPL)? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        demo_template='''Given that {impact}, what impact can this lead on Microsoft Corporation (MSFT) stock return ? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=['impact'],\n",
    "            template=demo_template\n",
    "            )\n",
    "\n",
    "        prompt.format(impact=i)\n",
    "        chain1=LLMChain(llm=llm,prompt=prompt)\n",
    "        news_impact_output=chain1.run(i)\n",
    "        print(news_impact_output)\n",
    "        negative_impact_list.append(news_impact_output)\n",
    "        negative_impact_dict[i]=news_impact_output\n",
    "        \n",
    "    data_negative = {'negative_news': negative_news_list, 'negative_impact': negative_impact_list}\n",
    "    df_negative = pd.DataFrame(data_negative)\n",
    "\n",
    "        # Optionally, set column names\n",
    "    df_negative.columns = ['negative_news', 'negative_impact']\n",
    "    df_negative['negative_impact'] = df_negative['negative_impact'].str.replace('\\n', '', regex=True)\n",
    "     \n",
    "    return df_negative ,negative_impact_dict              \n",
    "                     \n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "f7c255a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_dataframe,microsoft_negative_dict=neagtive_impact_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7b305d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"NVDA_news_yahoo_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed21136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9fbcb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_impact_analysis(df):\n",
    "    positive_news=df[df['sentiment']=='positive']\n",
    "    positive_news = positive_news.sort_values(by='confidence', ascending=False)\n",
    "    positive_news=positive_news.head(3)\n",
    "    positive_news_list=positive_news['desc'].tolist()                   \n",
    "    positive_impact_list=[] \n",
    "    positive_impact_dict={}   \n",
    "    for i in positive_news_list:\n",
    "        #print(str(i))\n",
    "        #demo_template='''Please clasify this content {docs} into three categories: financial-related contents, operational-related contents, brand reputational-related contents, and others.'''\n",
    "        demo_template='''Given that {impact}, what impact can this lead on NVIDIA Corporation (NVDA)? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        #demo_template='''Given that {impact}, what impact can this lead on Apple Inc(AAPL)? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        #demo_template='''Given that {impact}, what impact can this lead on Microsoft Corporation (MSFT) ? Please give me 3 most important bullet points and each one with clear and concise reasoning.''' \n",
    "        \n",
    "        #demo_template='''Given that {impact}, what impact can this lead to? Please give me 3 most important bullet points and each one with clear and concise reasoning.'''\n",
    "        prompt=PromptTemplate(\n",
    "            input_variables=['impact'],\n",
    "            template=demo_template\n",
    "            )\n",
    "\n",
    "        prompt.format(impact=i)\n",
    "        chain1=LLMChain(llm=llm,prompt=prompt)\n",
    "        news_impact_output=chain1.run(i)\n",
    "        print(news_impact_output)\n",
    "        positive_impact_list.append(news_impact_output)\n",
    "        positive_impact_dict[i]=positive_impact_dict\n",
    "        \n",
    "    data_positive = {'positive_news': positive_news_list, 'positive_impact': positive_impact_list}\n",
    "    df_positive = pd.DataFrame(data_positive)\n",
    "\n",
    "        # Optionally, set column names\n",
    "    df_positive.columns = ['positive_news', 'positive_impact']\n",
    "    df_positive['positive_impact'] = df_positive['positive_impact'].str.replace('\\n', '', regex=True)\n",
    "         \n",
    "    return df_positive,positive_impact_dict               \n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "49c85ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_pos_dataframe,nvidia_positive_dict=positive_impact_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "ea844a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"miltview_MSFT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882de461",
   "metadata": {},
   "source": [
    "### mutli_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "27c65517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_angle_summary(df,n,company_name):\n",
    "    model_multi = INSTRUCTOR('hkunlp/instructor-base')\n",
    "    embeddings = model_multi.encode(df['desc'])\n",
    "    clustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=5)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    print(cluster_assignment)\n",
    "    df['clusters']=cluster_assignment\n",
    "    key={}\n",
    "    df_1=df.head(50)\n",
    "    df_1=df_1[df_1[\"clusters\"]==n]\n",
    "    prompt_template=\"\"\"Based on the given text {text}, give me one summary of the text, be consistent and logical.\"\"\"\n",
    "   # prompt=PromptTemplate.from_template(prompte_template)\n",
    "    \n",
    "    BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                            input_variables=[\"text\"])\n",
    "    sentence_list = df_1['desc'].tolist()\n",
    "    from langchain.docstore.document import Document\n",
    "    docs = [Document(page_content=t) for t in sentence_list]\n",
    "    docs\n",
    "    chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "    output_summary = chain.run(docs)\n",
    "\n",
    "    wrapped_text = textwrap.fill(output_summary, \n",
    "                                 width=100,\n",
    "                                 break_long_words=False,\n",
    "                                 replace_whitespace=False)\n",
    "    #text_without_newlines = output_summary.replace('\\n', '')\n",
    "    key[company_name]=output_summary\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_multi_view1=(df,0,'Apple')\n",
    "apple_multi_view1=(df,1,'Apple')\n",
    "apple_multi_view1=(df,2,'Apple')\n",
    "apple_multi_view1=(df,3,'Apple')\n",
    "apple_multi_view1=(df,4,'Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "2bf74a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi=df[df[\"clusters\"]==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'cluster0': apple_multi_view1,\n",
    "    'cluster1': apple_multi_view2,\n",
    "    'cluster2': apple_multi_view3,\n",
    "    'cluster3': apple_multi_view4,\n",
    "    'cluster4': apple_multi_view5\n",
    "}\n",
    "\n",
    "df_mul = pd.DataFrame(data)\n",
    "df_mutli= df_mul.transpose()\n",
    "df_mutli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a95c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a69b69",
   "metadata": {},
   "source": [
    "## summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8db7de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain = load_summarize_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "712fa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_macro_sent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "ba8c0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df):\n",
    "    key={}\n",
    "    df_1=df.head(50)\n",
    "    #prompt_template = \"\"\"Write a concise bullet point summary of minimum 200 and maximum 250 words of the entity NVIDIA Corporation (NVDA) of the following:\n",
    "    #{text}\n",
    "    #CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "    #prompt_template =\"\"\"Write a concise bullet point summary maximum of 50 words of the industry finance of the following:\n",
    "    #{text}\n",
    "    #CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "    prompt_template = \"\"\"Write a summary of the macro economic enivronment minimum of 200 words based on the following news articles:\n",
    "    {text}\n",
    "    SUMMARY:\"\"\"\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                            input_variables=[\"text\"])\n",
    "    sentence_list = df_1['Description'].tolist()\n",
    "    #sentence_list = df_1['desc'].tolist()\n",
    "    from langchain.docstore.document import Document\n",
    "    docs = [Document(page_content=t) for t in sentence_list]\n",
    "    docs\n",
    "    chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "    output_summary = chain.run(docs)\n",
    "\n",
    "    wrapped_text = textwrap.fill(output_summary, \n",
    "                                 width=100,\n",
    "                                 break_long_words=False,\n",
    "                                 replace_whitespace=False)\n",
    "    #text_without_newlines = output_summary.replace('\\n', '')\n",
    "    key['200']=output_summary\n",
    "    \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_50=summary(df)\n",
    "macro_200=summary(df)\n",
    "macro_300=summary(df)\n",
    "series1 = pd.Series(macro_50)\n",
    "series2 = pd.Series(macro_200)\n",
    "#series3 = pd.Series(nvidia_300)\n",
    "df_43= pd.concat([series1, series2], axis=0)\n",
    "df_43.to_csv(\"macro_summary_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88bd67",
   "metadata": {},
   "source": [
    "## Chatbot question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "d70acc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_embeddings():\n",
    "    pdfreader = PdfReader('data.pdf')\n",
    "    raw_text = ''\n",
    "    for i, page in enumerate(pdfreader.pages):\n",
    "        content = page.extract_text()\n",
    "        if content:\n",
    "            raw_text += content\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 250,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = len\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_text(raw_text)\n",
    "    df_texts = pd.DataFrame({'id': range(1, len(text_chunks) + 1), 'text': text_chunks})\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(df_texts['text'].tolist())\n",
    "    id_array = df_texts['id'].to_numpy()\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(384))\n",
    "    index.add_with_ids(embeddings, id_array)\n",
    "    faiss.write_index(index, 'finance_index')\n",
    "    return df_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c201c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts=chatbot_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "ce2ec58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_query(query,df_texts):\n",
    "    index = faiss.read_index('finance_index')\n",
    "    t=time.time()\n",
    "    query_vector = model.encode([query])\n",
    "    k = 20\n",
    "    top_k = index.search(query_vector, k)\n",
    "    ids_to_match=ans[1][0].tolist()\n",
    "    matched_texts = df_texts[df_texts['id'].isin(ids_to_match)]['text'].tolist()\n",
    "    matched_texts\n",
    "    from langchain.docstore.document import Document\n",
    "    docs = [Document(page_content=t) for t in matched_texts]\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    answer=chain.run(input_documents=docs, question=ans)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_query(\"what is the gross margin for the june quarter?\",df_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56c0f4",
   "metadata": {},
   "source": [
    "### Retrace news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beccc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_retrace(query, model, index, k=10, threshold=0.5):\n",
    "    # Assuming the vectors are already normalized and the index is an IndexFlatIP\n",
    "    query_vector = model.encode([query])\n",
    "    faiss.normalize_L2(query_vector)  # Normalize the query vector if it isn't already\n",
    "    D, I = index.search(query_vector, k)\n",
    "    \n",
    "    # Apply the threshold to filter results\n",
    "    mask = D[0] >= threshold\n",
    "    I_filtered = I[0][mask]\n",
    "    D_filtered = D[0][mask]\n",
    "    \n",
    "    return D_filtered, I_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrace():\n",
    "    df=pd.read_csv('AAPL_news_yahoo_sent.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df_curr = df[df['date'].dt.day == 29]\n",
    "    df_curr = df_curr.head(4)\n",
    "    df_curr\n",
    "    start_date = pd.Timestamp('2023-10-20 00:00:00')\n",
    "    end_date = pd.Timestamp('2023-10-29 00:00:00')\n",
    "    df_hist = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    df_hist['id'] = range(1, len(df_hist) + 1)\n",
    "    id_array = df_hist['id'].to_numpy()\n",
    "    embeddings = model.encode(df_hist['desc'].tolist())\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(1024))\n",
    "    index.add_with_ids(embeddings, id_array)\n",
    "    added_descriptions = set()\n",
    "    results = [] \n",
    "    for desc in df_curr['desc']:\n",
    "        D, ids_to_match = search(desc, model, index)\n",
    "        if len(ids_to_match) == 0:\n",
    "            continue  \n",
    "    matched_texts = df_hist.loc[df_hist['id'].isin(ids_to_match), 'desc'].tolist()\n",
    "    matched_texts = [text for text in matched_texts if text not in added_descriptions]\n",
    "    added_descriptions.update(matched_texts)\n",
    "    filtered_df = df_hist[df_hist['id'].isin(ids_to_match)]\n",
    "    new_df = filtered_df[['desc', 'date','sentiment','confidence']].copy() \n",
    "    new_df = new_df[~new_df['desc'].isin([desc])]\n",
    "    new_df['original_desc'] = desc\n",
    "    results.append(new_df)\n",
    "    final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    date_range = pd.date_range(start=final_results_df['date'].min(), end=final_results_df['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "667dca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_retrace():\n",
    "    date_range = pd.date_range(start=final_results_df['date'].min(), end=final_results_df['date'].max())\n",
    "    sentiment_counts = final_results_df.groupby(['date', 'sentiment']).size().reset_index(name='count')\n",
    "    complete_sentiment_counts = (\n",
    "        sentiment_counts.set_index(['date', 'sentiment'])\n",
    "        .reindex(pd.MultiIndex.from_product([date_range, final_results_df['sentiment'].unique()], names=['date', 'sentiment']), fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    color_map = {\n",
    "        'negative': 'red',\n",
    "        'neutral': 'grey',\n",
    "        'positive': 'green'\n",
    "    }\n",
    "    ordered_sentiments = ['negative', 'neutral', 'positive']\n",
    "    complete_sentiment_counts['sentiment'] = pd.Categorical(complete_sentiment_counts['sentiment'], categories=ordered_sentiments, ordered=True)\n",
    "    fig = px.line(\n",
    "        complete_sentiment_counts,\n",
    "        x='date',\n",
    "        y='count',\n",
    "        color='sentiment',\n",
    "        title='Sentiment Counts Over Time For Apple Stocks',\n",
    "        labels={'count': 'Number of Occurrences', 'date': 'Date'},\n",
    "        color_discrete_map=color_map  # Use the color map for discrete colors based on sentiment\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        mode='lines+markers',\n",
    "        marker=dict(size=8)\n",
    "    )\n",
    "    for i, sentiment in enumerate(ordered_sentiments):\n",
    "        fig.for_each_trace(\n",
    "            lambda t, pattern=i: t.update(line=dict(dash=['solid', 'dot', 'dash'][pattern])) if t.name == sentiment else (),\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0, complete_sentiment_counts['count'].max() + 1])\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cdf157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c32958b",
   "metadata": {},
   "source": [
    "## Value chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688eaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AAPL_news_yahoo_sent.csv')\n",
    "def query(payload):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    for i in range(107,170):\n",
    "          output = query({\n",
    "            \"inputs\": df['feed'][i],\n",
    "          })\n",
    "          loc_words = [entry['word'] for entry in output if entry['entity_group'] == 'ORG']\n",
    "          coy_list.append(list(dict.fromkeys(loc_words)))\n",
    "          print(i)\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Gladiator/microsoft-deberta-v3-large_ner_conll2003\"\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# coy_list = []\n",
    "for i in range(107,170):\n",
    "  output = query({\n",
    "    \"inputs\": df['feed'][i],\n",
    "  })\n",
    "  loc_words = [entry['word'] for entry in output if entry['entity_group'] == 'ORG']\n",
    "  coy_list.append(list(dict.fromkeys(loc_words)))\n",
    "  print(i)\n",
    "    \n",
    "filtered_list = [sublist for sublist in coy_list if 'Bloomberg' not in sublist]\n",
    "\n",
    "# Step 1: Flatten the list of lists into a single list\n",
    "flat_list = [company for sublist in filtered_list for company in sublist]\n",
    "\n",
    "# Step 2: Count the occurrences of each company using a dictionary\n",
    "company_counts = {}\n",
    "for company in flat_list:\n",
    "    if company in company_counts:\n",
    "        company_counts[company] += 1\n",
    "    else:\n",
    "        company_counts[company] = 1\n",
    "\n",
    "# Step 3: Print the unique counts of companies\n",
    "for company, count in company_counts.items():\n",
    "    print(f\"{company}: {count}\")\n",
    "\n",
    "out = pd.DataFrame(company_counts.items(), columns=['Key','Value']).sort_values('Key')\n",
    "out.to_csv('AAPL_links.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf92087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723844f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74288adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72c706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93fc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1f79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c52b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9ac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b0744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1414813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd0d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b76ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd45bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34620f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2ca2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0c578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6163f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32807535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb239182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
